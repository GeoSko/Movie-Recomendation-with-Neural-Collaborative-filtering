# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gtJgivTVWDIXDIWnv2W0VWjbtWb0dBmC
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib as plt

df = pd.read_csv('u.data', sep = '\t')

# Comverting the dataset

# users-movies array
df.columns = ['user','movie','rate','timestamp']
df = df.drop(['timestamp'], axis = 1)
df.head()

df = df.pivot(index='user', columns='movie', values='rate')   
df.head()

# Histogram of users mean 
mean_plot = means.plot.hist(bins=20 )
mean_plot.set_xlabel("User's rating mean")
mean_plot.set_ylabel("Number of users")

# Centering the values subtracting the mean
df= df.sub(df.mean(axis=1),axis=0)
df.head()

#  Fill NaN values with zero(the mean)
df.fillna(value=0, inplace= True)
df.head()

# ÎŸne hot user vectors
users_one_hot = pd.get_dummies(df.index)
users_one_hot.head()

#Define Input and Output
inputs = users_one_hot
outputs = df

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import mean_squared_error,mean_absolute_error
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import regularizers
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from keras import backend as K
import numpy as np

# normalization
inputs = StandardScaler().fit_transform(X=inputs)
outputs = StandardScaler().fit_transform(X=outputs)

# Model configuration

optimizer = SGD()
num_folds = 5
verbosity = 1

loss_function = mean_absolute_error
val_split = 0.2
num_epochs = 100
num_neurons = 250
learning_rate = 0.001
momentum_factor = 0.1

reguralization_parameter=0.5
decay_factor = 0.9

# Define per-fold score containers
acc_per_fold = []
loss_per_fold = []

# Define the K-fold Cross Validator
kfold = KFold(n_splits=num_folds, shuffle=True)

#RMSE and MAE Loss Functions
def rmse(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_pred - y_true)))
def mae(y_true, y_pred):
        return K.mean(abs(y_pred - y_true))



# K-fold Cross Validation model evaluation
fold_no = 1

for train, test in kfold.split(inputs,outputs):
    #Create Model
    model = Sequential()

    #Reguralization
    kernel_reguralizer=regularizers.l1(reguralization_parameter)

    # Define the model architecture
    model.add(Dense(60, activation="tanh", input_dim=943))
    model.add(Dense(30, activation="relu", input_dim=60))
    model.add(Dense(20, activation="relu", input_dim=30))
    model.add(Dense(1682, activation="linear", input_dim=20))
    
    
    # Compile the model
    SGD(lr=learning_rate, momentum=momentum_factor, decay=decay_factor, nesterov=True) 
    
    model.compile(loss=loss_function, optimizer='sgd', metrics=[rmse])

     # Generate a print
    print('------------------------------------------------------------------------')
    print(f'Training for fold {fold_no} ...')

    #Early stopping monitor
    monitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience=10, verbose=1)

    # Fit data to model
    history = model.fit(inputs[train], outputs[train], epochs=num_epochs,batch_size=1 , validation_split=0.2, callbacks=[monitor])

    # Generate generalization metrics
    scores = model.evaluate(inputs[test], outputs[test], verbose=1)
    
    
    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]}')
    acc_per_fold.append(scores[1])
    loss_per_fold.append(scores[0])

    # Increase fold number
    fold_no = fold_no + 1
    

print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> RMSE: {np.mean(acc_per_fold)}')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')

# summarize history for RMSE
plt.plot(history.history['rmse'])
plt.plot(history.history['val_rmse'])
plt.title('model RMSE')
plt.ylabel('RMSE')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model MAE')
plt.ylabel('MAE')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

history.history.